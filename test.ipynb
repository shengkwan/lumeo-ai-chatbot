{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    tags=[\"latest\"],\n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful translator. Translate the user sentence to Middle English.\"),\n",
    "    HumanMessage(content=\"I love programming.\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful translator. Translate the user sentence to Middle English.\"),\n",
    "    HumanMessage(content=\"I love programming.\"),\n",
    "]\n",
    "\n",
    "for token in llm.stream(messages):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant and subject matter expert in {topic}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for token in chain.stream(\n",
    "    {\n",
    "        \"topic\": \"Politics\",\n",
    "        \"input\": \"Is Donald Trump a good president of US? as compared to Joe Biden\",\n",
    "    }\n",
    "):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system_prompt = \"You are a helpful assistant and subject matter expert in {topic}.\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain = prompt | llm | StrOutputParser()\n",
    "prompt = prompt_template.invoke(\n",
    "    {\n",
    "        \"topic\": \"Math\",\n",
    "        \"input\": \"what is The Collatz Conjecture?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "for token in llm.stream(prompt):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-q5_K_M\",\n",
    "    # tags=[\"latest\"],\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        print(event)\n",
    "        for value in event.values():\n",
    "            print(value)\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain LangGraph Ollama - Build a Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:3b-instruct-q5_K_M\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = llm.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.stream([HumanMessage(content=\"What's my name?\")])\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "response = llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    print(f\"state[\\\"messages\\\"] in call_model: \\n{state[\"messages\"]}\")\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "state = app.get_state(config)\n",
    "print(f\"state in app.get_state(config) is: \\n{state}\")\n",
    "output[\"messages\"]  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "state = app.get_state(config)\n",
    "print(f\"state in app.get_state(config) is: \\n{state}\")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    print(f\"state in call_model: \\n{state}\")\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=60000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm.get_num_tokens_from_messages,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"who are you?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"\")\n",
    "    # print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama (born August 4, 1961, Honolulu, Hawaii, U.S.) is the 44th president of the United States (2009-17) and the first African American to hold the office. Before winning the presidency, Obama represented Illinois in the U.S. Senate (2005-08). He was the third African American to be elected to that body since the end of Reconstruction (1877). In 2009 he was awarded the Nobel Peace ... Barack Obama, the former Illinois senator and two-term commander-in-chief, has kept busy since he left office in 2017. Barack Obama served as the 44th president of the United States (2009-17) and was the first African American to hold that post. A member of the Democratic Party, Obama previously represented Illinois in the U.S. Senate from 2005 to 2008. He was honoured with the Nobel Peace Prize in 2009. Barack Obama - 44th President, Political Career, Legacy: In 1996 he was elected to the Illinois Senate, where, most notably, he helped pass legislation that tightened campaign finance regulations, expanded health care to poor families, and reformed criminal justice and welfare laws. Barack Obama's presidency from 2009 to 2017 was marked by transformative policies, bold decisions, and historic achievements that left a lasting impact on the United States and the world. From navigating the country through one of the worst economic recessions in history to championing social justice and tackling global challenges like climate change, Obama's leadership was defined by his ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "result = search.invoke(\"who is Obama?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "message = ToolMessage(content=\"Hi\", tool_call_id=123)\n",
    "message.content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults(output_format='json')\n",
    "\n",
    "search_results = json.loads(search.invoke(\"Malaysia weather?\"))\n",
    "print(json.dumps(search_results[:2], indent=2))\n",
    "urls = [item[\"link\"] for item in search_results[:2]]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults(output_format='json', backend='news')\n",
    "\n",
    "result = search.invoke(\"Current status of Xi Jin Ping visiting to Malaysia\")\n",
    "\n",
    "# for item in result:\n",
    "#     print(item['link'])\n",
    "\n",
    "print(json.dumps(json.loads(result), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "tool = DuckDuckGoSearchResults()\n",
    "agent_chain = create_react_agent(model=llm, tools=[tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\"messages\": [(\"user\", \"what is the date today in Malaysia?\")]}\n",
    "result = agent_chain.invoke(user_input)\n",
    "result[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Browse Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import (\n",
    "    create_async_playwright_browser,  # A synchronous browser is available, though it isn't compatible with jupyter.\\n\",\t  },\n",
    ")\n",
    "\n",
    "# This import is required only for jupyter notebooks, since they have their own eventloop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_browser = create_async_playwright_browser()\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "tools = toolkit.get_tools()\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "pprint(tools_by_name)\n",
    "navigate_tool = tools_by_name[\"navigate_browser\"]\n",
    "get_elements_tool = tools_by_name[\"get_elements\"]\n",
    "extract_text_tool = tools_by_name[\"extract_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await navigate_tool.arun(\n",
    "    {\"url\": \"https://www.msn.com/en-us/money/markets/malaysia-cbank-says-us-tariffs-will-have-impact-but-economy-is-diversified/ar-AA1Cz3n2\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = await extract_text_tool.arun(\n",
    "    {}\n",
    ")\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = await extract_text_tool.arun(\n",
    "    {}\n",
    ")\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await get_elements_tool.arun(\n",
    "    {\"selector\": \"body\", \"attributes\": [\"innerText\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the agent wants to remember the current webpage, it can use the `current_webpage` tool\n",
    "await tools_by_name[\"current_webpage\"].arun({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search_tool = DuckDuckGoSearchResults()\n",
    "print(search_tool.name)\n",
    "print(search_tool.description)\n",
    "# all_tools = tools + [search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([search_tool])\n",
    "\n",
    "result = llm_with_tools.invoke(\"Hi\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:3b-instruct-q5_K_M\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "agent_executor = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=[search_tool], \n",
    "    checkpointer=memory, \n",
    "    prompt=\"\"\"You are a helpful assistant. Only use tools when necessary. Please think if the given user message require calling tools. If no, just answer the user message directly without calling the tools.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"How are you?\")]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "urls = [\"https://www.accuweather.com/en/my/kuala-lumpur/233776/weather-forecast/233776\", \"https://www.weather25.com/asia/malaysia?page=today\"]\n",
    "\n",
    "async def scrape_single_url(url):\n",
    "    try:\n",
    "        await navigate_tool.ainvoke({\"url\": url})\n",
    "        content = await get_elements_tool.ainvoke({\"selector\": \"body\"})\n",
    "        return f\"URL: {url}\\nContent:\\n{content}\\n\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed to scrape {url}: {str(e)}\"\n",
    "\n",
    "scraping_tasks = [scrape_single_url(url) for url in urls]\n",
    "scraped_contents = await asyncio.gather(*scraping_tasks)\n",
    "\n",
    "result = \"\\n\\n\".join(scraped_contents)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search + Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.agents import Tool\n",
    "\n",
    "ddg_search = DuckDuckGoSearchResults()\n",
    "# print(ddg_search.description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability. Before using the tools, think carefully if you really need to call the tool to form a response. If you don't need the tool, just proceed to generate response without calling the tool.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:3b-instruct-q5_K_M\",\n",
    "    # model='qwen3:4b',\n",
    "    temperature=0.5\n",
    ")\n",
    "llm_with_tools = llm.bind_tools([ddg_search])\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    # print(prompt)\n",
    "    # response = llm.invoke(prompt)\n",
    "    return {\"messages\": [llm_with_tools.invoke(prompt)]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[ddg_search])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = \"Hi there! My name is Will.\"\n",
    "\n",
    "# # The config is the **second positional argument** to stream() or invoke()!\n",
    "# events = graph.stream(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "#     config,\n",
    "#     stream_mode=\"values\",\n",
    "# )\n",
    "# for event in events:\n",
    "#     event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search and Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from typing import Optional\n",
    "from langchain_core.tools.base import ArgsSchema\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class SearchAndScrapeToolInput(BaseModel):\n",
    "    query: str = Field(description=\"Web search query string retrieved from user input\")\n",
    "\n",
    "# Setup search tool\n",
    "search_tool = DuckDuckGoSearchResults(output_format='json')\n",
    "\n",
    "# This import is required only for jupyter notebooks, since they have their own eventloop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply() \n",
    "\n",
    "# Setup Playwright tools\n",
    "async_browser = create_async_playwright_browser()\n",
    "playwright_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "navigate_tool = {tool.name: tool for tool in playwright_toolkit.get_tools()}[\"navigate_browser\"]\n",
    "get_elements_tool = {tool.name: tool for tool in playwright_toolkit.get_tools()}[\"get_elements\"]\n",
    "\n",
    "# Define your custom tool\n",
    "class SearchAndScrapeTool(BaseTool):\n",
    "    name: str = \"search_and_scrape_websites\"\n",
    "    description: str = \"Use this to search for any online and latest info. Input should be a search query string.\"\n",
    "    return_direct: bool = True\n",
    "    args_schema: Optional[ArgsSchema] = SearchAndScrapeToolInput\n",
    "\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        # Step 1: Search websites\n",
    "        search_results = json.loads(search_tool.invoke(query))\n",
    "        urls = [item[\"link\"] for item in search_results[:2]]  # Only take top 2\n",
    "\n",
    "        # Step 2: Scrape websites\n",
    "        scraped_contents = []\n",
    "        for url in urls:\n",
    "            try:\n",
    "                navigate_tool.invoke({\"url\": url})\n",
    "                content = get_elements_tool.invoke({\"selector\": \"body\"})  # Scrape page body\n",
    "                scraped_contents.append(f\"URL: {url}\\nContent:\\n{content}\\n\")\n",
    "            except Exception as e:\n",
    "                scraped_contents.append(f\"Failed to scrape {url}: {str(e)}\")\n",
    "\n",
    "        return \"\\n\\n\".join(scraped_contents)\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"Async not implemented for this simple tool.\")\n",
    "\n",
    "# Instantiate\n",
    "tool = SearchAndScrapeTool()\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.tools import tool\n",
    "# from langchain_community.tools import DuckDuckGoSearchResults\n",
    "# from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "# from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "\n",
    "# # Setup search tool\n",
    "# search_tool = DuckDuckGoSearchResults()\n",
    "\n",
    "# # This import is required only for jupyter notebooks, since they have their own eventloop\n",
    "# import nest_asyncio\n",
    "\n",
    "# nest_asyncio.apply() \n",
    "\n",
    "# # Setup Playwright tools\n",
    "# async_browser = create_async_playwright_browser()\n",
    "# playwright_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "# navigate_tool = {tool.name: tool for tool in playwright_toolkit.get_tools()}[\"navigate_browser\"]\n",
    "# get_elements_tool = {tool.name: tool for tool in playwright_toolkit.get_tools()}[\"get_elements\"]\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def search_and_srape_web(query: str) -> str:\n",
    "#     \"\"\"Use this to search the web for any online and latest info. Input should be a search query string.\"\"\"\n",
    "#     # Step 1: Search websites\n",
    "#     search_result = search_tool.invoke({\"query\": query})\n",
    "#     urls = [r[\"href\"] for r in search_result[\"results\"][:2]]  # Only take top 2\n",
    "\n",
    "#     # Step 2: Scrape websites\n",
    "#     scraped_contents = []\n",
    "#     for url in urls:\n",
    "#         try:\n",
    "#             navigate_tool.invoke({\"url\": url})\n",
    "#             content = get_elements_tool.invoke({\"selector\": \"body\"})  # Scrape page body\n",
    "#             scraped_contents.append(f\"URL: {url}\\nContent:\\n{content}\\n\")\n",
    "#         except Exception as e:\n",
    "#             scraped_contents.append(f\"Failed to scrape {url}: {str(e)}\")\n",
    "\n",
    "#     return \"\\n\\n\".join(scraped_contents)\n",
    "\n",
    "# tools = [search_and_srape_web]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from typing import Optional\n",
    "import asyncio\n",
    "from langchain_core.tools.base import ArgsSchema\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class SearchAndScrapeToolInput(BaseModel):\n",
    "    query: str = Field(\n",
    "        description=(\n",
    "            \"A concise web search query reformulated from the user's input. \"\n",
    "            \"It should include key nouns and entities (e.g. topics, locations) \"\n",
    "            \"and temporal terms (e.g. 'today', 'this week') if mentioned. \"\n",
    "            \"Omit unnecessary words like 'how', 'is', 'what', etc. \"\n",
    "            \"Example: From 'How is the status of Ukraine-Russia conflict today?', \"\n",
    "            \"extract 'Ukraine-Russia conflict status today'.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Setup search tool\n",
    "search_tool = DuckDuckGoSearchResults(output_format='list', backend='news')\n",
    "\n",
    "# This import is required only for jupyter notebooks, since they have their own eventloop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply() \n",
    "\n",
    "# Setup Playwright tools\n",
    "async_browser = create_async_playwright_browser()\n",
    "playwright_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "navigate_tool = {tool.name: tool for tool in playwright_toolkit.get_tools()}[\"navigate_browser\"]\n",
    "get_elements_tool = {tool.name: tool for tool in playwright_toolkit.get_tools()}[\"get_elements\"]\n",
    "\n",
    "class SearchAndScrapeToolAsync(BaseTool):\n",
    "    name: str = \"search_and_scrape_websites_async\"\n",
    "    description: str = \"Use this to search for any latest news if the news is out of your scope within your knowledge. Input should be a search query string.\"\n",
    "    return_direct: bool = True\n",
    "    args_schema: Optional[ArgsSchema] = SearchAndScrapeToolInput\n",
    "\n",
    "    def _run(self, query: str):\n",
    "        raise NotImplementedError(\"Use async version only.\")\n",
    "\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        # Step 1: Search\n",
    "        search_results = await search_tool.ainvoke(query)\n",
    "        urls = [item[\"link\"] for item in search_results]\n",
    "        # Step 2: Scrape in parallel\n",
    "        async def scrape_single_url(url):\n",
    "            try:\n",
    "                await navigate_tool.ainvoke({\"url\": url})\n",
    "                content = await get_elements_tool.ainvoke({\"selector\": \"body\", \"attributes\": [\"innerText\"]})\n",
    "                content = json.loads(content)\n",
    "                print(f\"Print statement - Content: \\n{content}\")\n",
    "                return f\"URL: {url}\\nContent:\\n{content}\\n\"\n",
    "            except Exception as e:\n",
    "                return f\"\"\n",
    "                # return None\n",
    "\n",
    "        scraping_tasks = [scrape_single_url(url) for url in urls]\n",
    "        scraped_contents = await asyncio.gather(*scraping_tasks)\n",
    "\n",
    "        return \"\\n\\n\".join(scraped_contents)\n",
    "\n",
    "# Instantiate\n",
    "tool = SearchAndScrapeToolAsync()\n",
    "tools = [tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:3b-instruct-q5_K_M\",\n",
    "    temperature=0.5\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is the impact of the newly implemented US tariff on Malaysia?\"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = await llm_with_tools.ainvoke(messages)\n",
    "\n",
    "print(ai_msg.tool_calls)\n",
    "\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    tool_msg = await tool.ainvoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_with_tools.invoke(messages)\n",
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph Tool Call Chatbot (decoupled llm with tools and single llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# tavily_api_key = os.environ[\"TAVILY_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "who is the current president of US?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (8d00be7f-1b5a-468a-8062-ac8e760997df)\n",
      " Call ID: 8d00be7f-1b5a-468a-8062-ac8e760997df\n",
      "  Args:\n",
      "    query: current president USA\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"current president USA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Presidents, vice presidents, and first ladies - USAGov\", \"url\": \"https://www.usa.gov/presidents\", \"content\": \"The president of the United States is the: U.S. head of state; Chief executive of the federal government; Commander-in-Chief of the armed forces; Current president. The 47th and current president of the United States is Donald John Trump. He was sworn into office on January 20, 2025. Former U.S. presidents. The United States has had 46 former U\", \"score\": 0.86331123, \"raw_content\": null}, {\"title\": \"President of the United States - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/President_of_the_United_States\", \"content\": \"The president of the United States (POTUS) [B] is the head of state and head of government of the United States. ... Donald Trump is the 47th and current president since January 20, 2025. [21] History and development. Origins.\", \"score\": 0.7825466, \"raw_content\": null}, {\"title\": \"List of presidents of the United States - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\", \"content\": \"The president of the United States is the head of state and head of government of the United States, [1] indirectly elected to a four-year term via the Electoral College. [2] Under the U.S. Constitution, the officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. [3] The\", \"score\": 0.5530473, \"raw_content\": null}, {\"title\": \"President Donald J. Trump - The White House\", \"url\": \"https://www.whitehouse.gov/administration/donald-j-trump/\", \"content\": \"In his first administration, President Trump passed record-setting tax cuts and regulation cuts, achieved energy independence, replaced NAFTA with the United-States-Mexico-Canada Agreement\", \"score\": 0.375043, \"raw_content\": null}, {\"title\": \"list of presidents of the United States - Encyclopedia Britannica\", \"url\": \"https://www.britannica.com/topic/Presidents-of-the-United-States-1846696\", \"content\": \"As the head of the government of the United States, the president is arguably the most powerful government official in the world. The president is elected to a four-year term via an electoral college system. Since the Twenty-second Amendment was adopted in 1951, the American presidency has been limited to a maximum of two terms.. Click on a president below to learn more about each presidency\", \"score\": 0.34886748, \"raw_content\": null}], \"response_time\": 1.82}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current president of the United States is Donald John Trump. He was sworn into office on January 20, 2025.\n"
     ]
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize Tavily Search Tool\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    include_answer=False,\n",
    ")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model='qwen2.5:3b-instruct-q5_K_M',\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, [tavily_search_tool])\n",
    "\n",
    "user_input = \"who is the current president of US?\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": user_input},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# wrapper = DuckDuckGoSearchAPIWrapper(time=\"d\")\n",
    "# ddg_search = DuckDuckGoSearchResults(\n",
    "#     api_wrapper=wrapper,\n",
    "#     output_format='string',\n",
    "#     backend='text'\n",
    "# )\n",
    "\n",
    "# Initialize Tavily Search Tool\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    include_answer=False,\n",
    ")\n",
    "\n",
    "# tools = [ddg_search]\n",
    "tools = [tavily_search_tool]\n",
    "\n",
    "llm = ChatOllama(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model='qwen2.5:3b-instruct-q5_K_M',\n",
    "    temperature=0.5\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot_with_tools(state: State):\n",
    "    print(\"Chatbot with tools is executing....\\n\")\n",
    "    print(\"State for chatbot with tools:\\n\")\n",
    "    for message in state[\"messages\"]:\n",
    "        message.pretty_print()\n",
    "    print(\"\\nEnd of state for chatbot with tools:\\n\")\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    print(f\"Response from chatbot with tools: {response} \\n\")\n",
    "    print(f\"Tool calls from chatbot with tools: {response.tool_calls} \\n\")\n",
    "    print(\"Chatbot with tools done executing\\n\")\n",
    "    if hasattr(response, \"tool_calls\") and len(response.tool_calls) <= 0:\n",
    "        # return {\"messages\": [AIMessage(content=\"\")]}\n",
    "        return None\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # print(f\"\\nState: {state[\"messages\"]}\")\n",
    "    print(\"Chatbot is executing....\\n\")\n",
    "    print(\"State for chatbot:\\n\")\n",
    "    for message in state[\"messages\"]:\n",
    "        message.pretty_print()\n",
    "    print(\"End of state for chatbot:\\n\")\n",
    "    print(\"Chatbot done executing\\n\")\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the tool node if the last message\n",
    "    has tool calls. Otherwise, route to the chatbot.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"chatbot\"\n",
    "\n",
    "graph_builder.add_node(\"chatbot_with_tools\", chatbot_with_tools)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "\n",
    "# tool_node = ToolNode(tools=[ddg_search])\n",
    "tool_node = ToolNode(tools=[tavily_search_tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot_with_tools\",\n",
    "    # tools_condition,\n",
    "    route_tools\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot_with_tools\")\n",
    "# graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around Duck Duck Go Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddg_search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot with tools is executing....\n",
      "\n",
      "State for chatbot with tools:\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi\n",
      "\n",
      "End of state for chatbot with tools:\n",
      "\n",
      "Response from chatbot with tools: content='Hello! How can I assist you today?' additional_kwargs={} response_metadata={'model': 'qwen2.5:3b-instruct-q5_K_M', 'created_at': '2025-05-07T14:23:54.3903461Z', 'done': True, 'done_reason': 'stop', 'total_duration': 715515900, 'load_duration': 43681500, 'prompt_eval_count': 952, 'prompt_eval_duration': 242793100, 'eval_count': 10, 'eval_duration': 421977800, 'message': Message(role='assistant', content='Hello! How can I assist you today?', images=None, tool_calls=None)} id='run-47acb9f9-3737-4d56-b0ae-3020cbb206af' usage_metadata={'input_tokens': 952, 'output_tokens': 10, 'total_tokens': 962} \n",
      "\n",
      "Tool calls from chatbot with tools: [] \n",
      "\n",
      "Chatbot with tools done executing\n",
      "\n",
      "Chatbot is executing....\n",
      "\n",
      "State for chatbot:\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi\n",
      "End of state for chatbot:\n",
      "\n",
      "Chatbot done executing\n",
      "\n",
      "Hello! How can I assist you today? Feel free to ask any questions or let me know if you need information on a particular topic.Chatbot with tools is executing....\n",
      "\n",
      "State for chatbot with tools:\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who is the current president of US?\n",
      "\n",
      "End of state for chatbot with tools:\n",
      "\n",
      "Response from chatbot with tools: content='' additional_kwargs={} response_metadata={'model': 'qwen2.5:3b-instruct-q5_K_M', 'created_at': '2025-05-07T14:24:13.404026Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2094257000, 'load_duration': 20023500, 'prompt_eval_count': 959, 'prompt_eval_duration': 830688600, 'eval_count': 26, 'eval_duration': 1239298100, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-1c659418-54ce-49a0-9742-80414c597354' tool_calls=[{'name': 'tavily_search', 'args': {'query': 'current president of United States'}, 'id': '296895b4-67af-482d-8f28-fee83a252201', 'type': 'tool_call'}] usage_metadata={'input_tokens': 959, 'output_tokens': 26, 'total_tokens': 985} \n",
      "\n",
      "Tool calls from chatbot with tools: [{'name': 'tavily_search', 'args': {'query': 'current president of United States'}, 'id': '296895b4-67af-482d-8f28-fee83a252201', 'type': 'tool_call'}] \n",
      "\n",
      "Chatbot with tools done executing\n",
      "\n",
      "Chatbot is executing....\n",
      "\n",
      "State for chatbot:\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who is the current president of US?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (296895b4-67af-482d-8f28-fee83a252201)\n",
      " Call ID: 296895b4-67af-482d-8f28-fee83a252201\n",
      "  Args:\n",
      "    query: current president of United States\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"current president of United States\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"President of the United States - Simple English Wikipedia, the free ...\", \"url\": \"https://simple.wikipedia.org/wiki/President_of_the_United_States\", \"content\": \"The president is also the head of the executive branch of the federal government of the United States and is the chairman of the presidential cabinet. [12] Donald J. Trump is the 47th and current president of the United States, in office since January 2025. [13]\", \"score\": 0.843393, \"raw_content\": null}, {\"title\": \"List of presidents of the United States - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\", \"content\": \"The president of the United States is the head of state and head of government of the United States, [1] indirectly elected to a four-year term via the Electoral College. [2] Under the U.S. Constitution, the officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. [3]\", \"score\": 0.5582553, \"raw_content\": null}, {\"title\": \"President Donald J. Trump - The White House\", \"url\": \"https://www.whitehouse.gov/administration/donald-j-trump/\", \"content\": \"President Trump built on his success in private life when he entered into politics and public service. He remarkably won the Presidency in his first ever run for any political office.\", \"score\": 0.5417246, \"raw_content\": null}, {\"title\": \"List of presidents of the United States | U.S. Presidents, Presidential ...\", \"url\": \"https://www.britannica.com/topic/Presidents-of-the-United-States-1846696\", \"content\": \"Click on a president below to learn more about each presidency through an interactive timeline. The table below the graphic provides a list of presidents of the United States, their birthplaces, political parties, and terms of office.\", \"score\": 0.2991927, \"raw_content\": null}, {\"title\": \"Presidents, vice presidents, and first ladies - USAGov\", \"url\": \"https://www.usa.gov/presidents\", \"content\": \"Learn about the duties of president, vice president, and first lady of the United States. Find out how to contact and learn more about current and past leaders.\", \"score\": 0.2835363, \"raw_content\": null}], \"response_time\": 1.54}\n",
      "End of state for chatbot:\n",
      "\n",
      "Chatbot done executing\n",
      "\n",
      "According to the information provided by the search results, Donald J. Trump is currently serving as the president of the United States, having taken office in January 2025.Chatbot with tools is executing....\n",
      "\n",
      "State for chatbot with tools:\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the most adverse impact of tariff imposed by US to all the other countries?\n",
      "\n",
      "End of state for chatbot with tools:\n",
      "\n",
      "Response from chatbot with tools: content='' additional_kwargs={} response_metadata={'model': 'qwen2.5:3b-instruct-q5_K_M', 'created_at': '2025-05-07T14:25:17.7609052Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2228532800, 'load_duration': 18204900, 'prompt_eval_count': 968, 'prompt_eval_duration': 838631900, 'eval_count': 30, 'eval_duration': 1362656900, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-1b5617aa-9022-4811-9ce8-6b73a809c39f' tool_calls=[{'name': 'tavily_search', 'args': {'query': 'most adverse impact of US tariffs on other countries'}, 'id': 'e33b6c20-899e-4ca3-9355-d9513cf48048', 'type': 'tool_call'}] usage_metadata={'input_tokens': 968, 'output_tokens': 30, 'total_tokens': 998} \n",
      "\n",
      "Tool calls from chatbot with tools: [{'name': 'tavily_search', 'args': {'query': 'most adverse impact of US tariffs on other countries'}, 'id': 'e33b6c20-899e-4ca3-9355-d9513cf48048', 'type': 'tool_call'}] \n",
      "\n",
      "Chatbot with tools done executing\n",
      "\n",
      "Chatbot is executing....\n",
      "\n",
      "State for chatbot:\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the most adverse impact of tariff imposed by US to all the other countries?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (e33b6c20-899e-4ca3-9355-d9513cf48048)\n",
      " Call ID: e33b6c20-899e-4ca3-9355-d9513cf48048\n",
      "  Args:\n",
      "    query: most adverse impact of US tariffs on other countries\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"most adverse impact of US tariffs on other countries\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"The Impact of US Tariff Policies and the Emerging Opportunities\", \"url\": \"https://moderndiplomacy.eu/2025/04/04/the-impact-of-us-tariff-policies-and-the-emerging-opportunities/\", \"content\": \"The US, under Trumps administration, implemented a series of tariff measures targeting various countries that had long enjoyed access to the vast American market with minimal trade barriers. The United States, under President Trumps administration, implemented a series of tariff measures targeting various countries that had long enjoyed access to the vast American market with minimal trade barriers. South Korea and Japan: While both countries have strategic economic ties with the US, they faced increased tariffs on certain exports, prompting negotiations to secure favorable trade agreements. EU-Asia Collaborations: The EU deepened trade relations with Asian countries, signing agreements with Japan and Vietnam to reduce tariff dependencies on the US.\", \"score\": 0.53181785, \"raw_content\": null}, {\"title\": \"Tariff tremors: The global impact of US trade policies\", \"url\": \"https://www.lseg.com/en/insights/ftse-russell/tariff-tremors-the-global-impact-of-us-trade-policies\", \"content\": \"We analyse three key channels of transmission to understand the effects of trade policy shifts: US revenue exposure of different geographic equity markets, export dependence of various economies, and volatility spillover effects from increased US equity market volatility. Combining both the analysis above would suggest that economies whose equity markets (companies in their equity indices) have high US revenue exposure, and their macroeconomic model has high export dependence could face the greatest risks from tariff-induced disruptions. These findings suggest that equity investors need to consider various transmission channels  revenue exposure of their equities to the US market, trade dependencies and equity market beta, when assessing risks tied to US trade policy.\", \"score\": 0.47154015, \"raw_content\": null}, {\"title\": \"US Tariffs: What's the Impact? | J.P. Morgan Research\", \"url\": \"https://www.jpmorgan.com/insights/global-research/current-events/us-tariffs\", \"content\": \"March 14: J.P. Morgan Research revises down U.S. GDP growth based on tariffs We anticipate this slide in sentiment to accelerate sharply into midyear, as a front-loaded lift in global industry fades and as the April tariff announcement weighs on business confidence broadly, said Bruce Kasman, chief global economist at J.P. Morgan. Given the import bill from China, this tariff alone amounts to an enormous $400bn tax hike on U.S. households and businesses before substitution, said Nora Szentivanyi, senior global economist at J.P. Morgan. J.P. Morgan Research has lowered its estimate for 2025 real GDP growth due to heightened trade policy uncertainty, the effect of existing tariffs and retaliatory measures by foreign trading partners.\", \"score\": 0.44956237, \"raw_content\": null}, {\"title\": \"Global Trade Update (April 2025): Escalating tariffs - the impact on ...\", \"url\": \"https://unctad.org/publication/global-trade-update-april-2025-escalating-tariffs-impact-small-and-vulnerable-economies\", \"content\": \"Global Trade Update (April 2025): Escalating tariffs  the impact on small and vulnerable economies | UN Trade and Development (UNCTAD) We help developing countries benefit from the global economy more fairly and effectively, providing data and analysis, facilitating consensus-building and offering technical assistance on issues related to trade and development. We provide reliable and timely data and statistics to help countries better understand trade and development trends and design more effective economic, environmental and social policies. Global Trade Update (April 2025): Escalating tariffs  the impact on small and vulnerable economies Global Trade Update (April 2025): Escalating tariffs  the impact on small and vulnerable economies Global Trade Update (April 2025): Escalating tariffs  the impact on small and vulnerable economies (UNCTAD/DITC/INF/2025/2)\", \"score\": 0.21740435, \"raw_content\": null}, {\"title\": \"The Macroeconomic Consequences of Import Tariffs and Trade Policy ...\", \"url\": \"https://www.imf.org/en/Publications/WP/Issues/2024/01/19/The-Macroeconomic-Consequences-of-Import-Tariffs-and-Trade-Policy-Uncertainty-543877\", \"content\": \"IMF at a Glance IMF Members IMF Financial Statements IMF Researchers Other IMF Events IMF reports and publications by country IMF Regional Reports IMF and Europe IMF Regional Office for Asia and the Pacific IMF Training About the IMF IMF Data Portal IMF Finances IMF NOTES IMF eLibrary IMF Working Papers IMF Publications Newsletter ### Republic of Korea: Selected Issues February 7, 2025 ### Grenada: 2024 Article IV Consultation-Press Release; Staff Report; and Statement by the Executive Director for Grenada February 4, 2025 ### Republic of Armenia: Technical Assistance Report-Personal Income Tax and Social Security Contribution Gaps January 31, 2025 ### South Africa: 2024 Article IV Consultation-Press Release; Staff Report; and Statement by the Executive Director for South Africa January 30, 2025\", \"score\": 0.09312415, \"raw_content\": null}], \"response_time\": 1.96}\n",
      "End of state for chatbot:\n",
      "\n",
      "Chatbot done executing\n",
      "\n",
      "Based on the information provided from academic articles and economic research, one of the most adverse impacts of US tariffs imposed on other countries is a decline in global trade and increased economic uncertainty for many economies. Here are some key points:\n",
      "\n",
      "1. **Economic Uncertainty**: Tariffs can lead to significant uncertainty among businesses and investors about future trading conditions. This can result in reduced investment, delayed business decisions, and overall slower economic growth.\n",
      "\n",
      "2. **Revenue Impact**: For countries that heavily rely on exports to the US market, tariffs can significantly reduce their revenue streams. For example, South Korea and Japan faced increased tariffs on certain exports, which prompted negotiations for favorable trade agreements.\n",
      "\n",
      "3. **Economic Growth Impacts**: Tariffs can have direct effects on GDP growth. The J.P. Morgan Research analysis suggests that US tariffs could lower GDP growth estimates due to heightened trade policy uncertainty and the tax implications of these tariffs on households and businesses.\n",
      "\n",
      "4. **Global Trade Disruption**: Tariffs often lead to retaliatory measures from other countries, potentially disrupting global supply chains and markets. This can cause disruptions in industries across various sectors.\n",
      "\n",
      "5. **Small and Vulnerable Economies**: The UNCTAD report highlights that small and vulnerable economies are disproportionately affected by tariffs, as they may have less ability to diversify their trade relationships or absorb the economic shocks.\n",
      "\n",
      "While specific adverse impacts might vary based on individual country's reliance on US markets and other factors, these general points provide a comprehensive view of how US tariffs can have significant adverse effects on global trade and economies.Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for msg, metadata in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, stream_mode=\"messages\"):\n",
    "        if msg.content and metadata[\"langgraph_node\"] == \"chatbot\":\n",
    "        # if msg.content:\n",
    "            print(msg.content, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# def stream_graph_updates(user_input: str):\n",
    "#     for msg, metadata in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, stream_mode=\"messages\"):\n",
    "#         if isinstance(msg, BaseMessage) and hasattr(msg, \"content\"):\n",
    "#             print(msg.content or \"\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ddg_search = DuckDuckGoSearchResults()\n",
    "tools = [ddg_search]\n",
    "llm = ChatOllama(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model='qwen3:4b',\n",
    "    temperature=0.5\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# llm = ChatOllama(model=\"llama3.2:3b\", temperature=0).bind_tools([])\n",
    "for chunk in llm_with_tools.stream(\"Tell me a joke\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph Tool Call Chatbot (only  llm with tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "ddg_search = DuckDuckGoSearchResults()\n",
    "tools = [ddg_search]\n",
    "llm = ChatOllama(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model='qwen2.5:3b-instruct-q5_K_M',\n",
    "    temperature=0.5\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot_with_tools(state: State):\n",
    "    # for message in state[\"messages\"]:\n",
    "    print(f\"State: {state[\"messages\"]} \\n\")\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# def chatbot(state: State):\n",
    "#     # print(f\"\\nState: {state[\"messages\"]}\")\n",
    "#     for message in state[\"messages\"]:\n",
    "#         message.pretty_print()\n",
    "#     return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot_with_tools\", chatbot_with_tools)\n",
    "# graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools=[ddg_search])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot_with_tools\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot_with_tools\")\n",
    "graph_builder.set_entry_point(\"chatbot_with_tools\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='df099224-4661-447f-8e2d-7da28f3c9e51')] \n",
      "\n",
      "snippet: Hello. I'm Catherine. Rob Hello. I'm Rob. Catherine We both started with what is probably the best-known greeting in English and one of the first words English language students learn, and that is ..., title: BBC Learning English - 6 Minute English / Hello, hello, link: https://www.bbc.com/learningenglish/english/features/6-minute-english/ep-180301, snippet: Customs and culture for saying hello. The origins for some of these ways to say \"hello\" are fascinating. First, hello was an alternate of the word hallo, which was an alternate of holla. Holla was once used in 14th century England to get someone to stop, but it is now an informal greeting as well. Howdy has history, too!, title: Different Ways to Say Hello in English and How to Respond - Duolingo Blog, link: https://blog.duolingo.com/hello-in-english/, snippet: Hello, my colleague. - This professional greeting is a respectful way to greet a coworker or someone you work with regularly. Hello, sir/madam. - This professional greeting is a formal and respectful way to greet someone, especially if you do not know their name. Hello, Mr./Mrs./Miss [name]. - This professional greeting is a formal and ..., title: 50+ Creative Ways to Say \"Hello\" in English - 7ESL, link: https://7esl.com/ways-to-say-hello/, snippet: Choosing the right greeting depends on whether you are at a formal meeting, hanging out with friends, or having fun with playful interactions. Instead of just saying the same boring 'hello!' you can explore funny ways to describe yourself. Or, you can just tweak your 'hello' in ear-catching ways. Check some interesting ways here!, title: 123 Cool And Different Ways To Say Hello And Greet People - Stylecraze, link: https://www.stylecraze.com/articles/ways-to-say-hello/State: [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='df099224-4661-447f-8e2d-7da28f3c9e51'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-05-04T09:46:09.1498994Z', 'done': True, 'done_reason': 'stop', 'total_duration': 896909600, 'load_duration': 19734100, 'prompt_eval_count': 181, 'prompt_eval_duration': 233943700, 'eval_count': 20, 'eval_duration': 642688600, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-352e540b-8e54-43c5-9d73-c22a42f86369', tool_calls=[{'name': 'duckduckgo_results_json', 'args': {'query': 'Hello'}, 'id': '521c0362-6b29-468f-9964-2c789bb76c92', 'type': 'tool_call'}], usage_metadata={'input_tokens': 181, 'output_tokens': 20, 'total_tokens': 201}), ToolMessage(content='snippet: Hello. I\\'m Catherine. Rob Hello. I\\'m Rob. Catherine We both started with what is probably the best-known greeting in English and one of the first words English language students learn, and that is ..., title: BBC Learning English - 6 Minute English / Hello, hello, link: https://www.bbc.com/learningenglish/english/features/6-minute-english/ep-180301, snippet: Customs and culture for saying hello. The origins for some of these ways to say \"hello\" are fascinating. First, hello was an alternate of the word hallo, which was an alternate of holla. Holla was once used in 14th century England to get someone to stop, but it is now an informal greeting as well. Howdy has history, too!, title: Different Ways to Say Hello in English and How to Respond - Duolingo Blog, link: https://blog.duolingo.com/hello-in-english/, snippet: Hello, my colleague. - This professional greeting is a respectful way to greet a coworker or someone you work with regularly. Hello, sir/madam. - This professional greeting is a formal and respectful way to greet someone, especially if you do not know their name. Hello, Mr./Mrs./Miss [name]. - This professional greeting is a formal and ..., title: 50+ Creative Ways to Say \"Hello\" in English - 7ESL, link: https://7esl.com/ways-to-say-hello/, snippet: Choosing the right greeting depends on whether you are at a formal meeting, hanging out with friends, or having fun with playful interactions. Instead of just saying the same boring \\'hello!\\' you can explore funny ways to describe yourself. Or, you can just tweak your \\'hello\\' in ear-catching ways. Check some interesting ways here!, title: 123 Cool And Different Ways To Say Hello And Greet People - Stylecraze, link: https://www.stylecraze.com/articles/ways-to-say-hello/', name='duckduckgo_results_json', id='183a3767-b9f9-4f3e-8883-fac73f6acae1', tool_call_id='521c0362-6b29-468f-9964-2c789bb76c92', artifact=[{'snippet': \"Hello. I'm Catherine. Rob Hello. I'm Rob. Catherine We both started with what is probably the best-known greeting in English and one of the first words English language students learn, and that is ...\", 'title': 'BBC Learning English - 6 Minute English / Hello, hello', 'link': 'https://www.bbc.com/learningenglish/english/features/6-minute-english/ep-180301'}, {'snippet': 'Customs and culture for saying hello. The origins for some of these ways to say \"hello\" are fascinating. First, hello was an alternate of the word hallo, which was an alternate of holla. Holla was once used in 14th century England to get someone to stop, but it is now an informal greeting as well. Howdy has history, too!', 'title': 'Different Ways to Say Hello in English and How to Respond - Duolingo Blog', 'link': 'https://blog.duolingo.com/hello-in-english/'}, {'snippet': 'Hello, my colleague. - This professional greeting is a respectful way to greet a coworker or someone you work with regularly. Hello, sir/madam. - This professional greeting is a formal and respectful way to greet someone, especially if you do not know their name. Hello, Mr./Mrs./Miss [name]. - This professional greeting is a formal and ...', 'title': '50+ Creative Ways to Say \"Hello\" in English - 7ESL', 'link': 'https://7esl.com/ways-to-say-hello/'}, {'snippet': \"Choosing the right greeting depends on whether you are at a formal meeting, hanging out with friends, or having fun with playful interactions. Instead of just saying the same boring 'hello!' you can explore funny ways to describe yourself. Or, you can just tweak your 'hello' in ear-catching ways. Check some interesting ways here!\", 'title': '123 Cool And Different Ways To Say Hello And Greet People - Stylecraze', 'link': 'https://www.stylecraze.com/articles/ways-to-say-hello/'}])] \n",
      "\n",
      "Here's a formatted answer to the original user question:\n",
      "\n",
      "There are several ways to say \"hello\" in English, and the right one to use depends on the context and the level of formality.\n",
      "\n",
      "Some common greetings include:\n",
      "\n",
      "* Hello, my colleague. (a respectful way to greet a coworker or someone you work with regularly)\n",
      "* Hello, sir/madam. (a formal and respectful way to greet someone, especially if you don't know their name)\n",
      "* Hello, Mr./Mrs./Miss [name]. (a formal and professional way to greet someone)\n",
      "\n",
      "You can also use more creative and playful ways to say \"hello\", such as:\n",
      "\n",
      "* Howdy! (which has a fun history and is often used in informal settings)\n",
      "* Hallo! (an alternate of the word hello that's been used for centuries)\n",
      "* Holla! (another old-fashioned way to get someone's attention, now used as an informal greeting)\n",
      "\n",
      "Ultimately, the choice of greeting depends on your relationship with the person you're addressing and the tone you want to convey.State: [HumanMessage(content='Check weather today in Kuala Lumpur', additional_kwargs={}, response_metadata={}, id='0f010442-d76c-4892-9033-25a34ad44c1a')] \n",
      "\n",
      "snippet: Kuala Lumpur, Malaysia - Current temperature and weather conditions. Detailed hourly weather forecast for today - including weather conditions, temperature, pressure, humidity, precipitation, dewpoint, wind, visibility, and UV index data., title: Weather today - Kuala Lumpur, Malaysia, link: https://www.weather-atlas.com/en/malaysia/kuala-lumpur, snippet: The temperature in Kuala Lumpur today in the early morning is 24  C. If you take into account factors such as wind, humidity and other weather conditions, the temperatures can feel like 27  C. The chance of rain in Kuala Lumpur in the morning is 74%, and the wind will blow at 4 km/h., title: Kuala Lumpur local weather (live): today, hourly weather, link: https://www.weather25.com/asia/malaysia/kuala-lumpur?page=today, snippet: Today's temperature is forecast to be COOLER than yesterday. Thunderstorms likely. Heavy and torrential downpours at times. High 93F. Winds WSW at 5 to 10 mph. Chance of rain 100%...., title: Kuala Lumpur, Malaysia Weather Conditions - Weather Underground, link: https://www.wunderground.com/weather/my/kuala-lumpur, snippet: Kuala Lumpur 7 day weather forecast including weather warnings, temperature, rain, wind, visibility, humidity and UV, title: Kuala Lumpur (Malaysia) weather - Met Office, link: https://weather.metoffice.gov.uk/forecast/w283fmqynState: [HumanMessage(content='Check weather today in Kuala Lumpur', additional_kwargs={}, response_metadata={}, id='0f010442-d76c-4892-9033-25a34ad44c1a'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-05-04T09:46:28.6750636Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1254810500, 'load_duration': 24698500, 'prompt_eval_count': 186, 'prompt_eval_duration': 433139500, 'eval_count': 24, 'eval_duration': 796446800, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-965c8a6d-f480-4490-83ab-54aabad87078', tool_calls=[{'name': 'duckduckgo_results_json', 'args': {'query': 'Kuala Lumpur weather today'}, 'id': 'db076e2c-8af5-4148-916c-aca6f5e62b2d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 186, 'output_tokens': 24, 'total_tokens': 210}), ToolMessage(content=\"snippet: Kuala Lumpur, Malaysia - Current temperature and weather conditions. Detailed hourly weather forecast for today - including weather conditions, temperature, pressure, humidity, precipitation, dewpoint, wind, visibility, and UV index data., title: Weather today - Kuala Lumpur, Malaysia, link: https://www.weather-atlas.com/en/malaysia/kuala-lumpur, snippet: The temperature in Kuala Lumpur today in the early morning is 24  C. If you take into account factors such as wind, humidity and other weather conditions, the temperatures can feel like 27  C. The chance of rain in Kuala Lumpur in the morning is 74%, and the wind will blow at 4 km/h., title: Kuala Lumpur local weather (live): today, hourly weather, link: https://www.weather25.com/asia/malaysia/kuala-lumpur?page=today, snippet: Today's temperature is forecast to be COOLER than yesterday. Thunderstorms likely. Heavy and torrential downpours at times. High 93F. Winds WSW at 5 to 10 mph. Chance of rain 100%...., title: Kuala Lumpur, Malaysia Weather Conditions - Weather Underground, link: https://www.wunderground.com/weather/my/kuala-lumpur, snippet: Kuala Lumpur 7 day weather forecast including weather warnings, temperature, rain, wind, visibility, humidity and UV, title: Kuala Lumpur (Malaysia) weather - Met Office, link: https://weather.metoffice.gov.uk/forecast/w283fmqyn\", name='duckduckgo_results_json', id='2ecf05bc-0cdf-4e21-b77b-1bdcffe515f7', tool_call_id='db076e2c-8af5-4148-916c-aca6f5e62b2d', artifact=[{'snippet': 'Kuala Lumpur, Malaysia - Current temperature and weather conditions. Detailed hourly weather forecast for today - including weather conditions, temperature, pressure, humidity, precipitation, dewpoint, wind, visibility, and UV index data.', 'title': 'Weather today - Kuala Lumpur, Malaysia', 'link': 'https://www.weather-atlas.com/en/malaysia/kuala-lumpur'}, {'snippet': 'The temperature in Kuala Lumpur today in the early morning is 24  C. If you take into account factors such as wind, humidity and other weather conditions, the temperatures can feel like 27  C. The chance of rain in Kuala Lumpur in the morning is 74%, and the wind will blow at 4 km/h.', 'title': 'Kuala Lumpur local weather (live): today, hourly weather', 'link': 'https://www.weather25.com/asia/malaysia/kuala-lumpur?page=today'}, {'snippet': \"Today's temperature is forecast to be COOLER than yesterday. Thunderstorms likely. Heavy and torrential downpours at times. High 93F. Winds WSW at 5 to 10 mph. Chance of rain 100%....\", 'title': 'Kuala Lumpur, Malaysia Weather Conditions - Weather Underground', 'link': 'https://www.wunderground.com/weather/my/kuala-lumpur'}, {'snippet': 'Kuala Lumpur 7 day weather forecast including weather warnings, temperature, rain, wind, visibility, humidity and UV', 'title': 'Kuala Lumpur (Malaysia) weather - Met Office', 'link': 'https://weather.metoffice.gov.uk/forecast/w283fmqyn'}])] \n",
      "\n",
      "The current weather in Kuala Lumpur today is mostly rainy with a high of 93F (33C) and winds blowing at 5-10 mph from the WSW. There's a 100% chance of rain throughout the day, with heavy and torrential downpours expected at times. The temperature feels like 27C due to factors such as wind, humidity, and other weather conditions.State: [HumanMessage(content='check weather today in Penang', additional_kwargs={}, response_metadata={}, id='b5e1f6d0-14a6-4569-b72d-df56d3b37267')] \n",
      "\n",
      "snippet: The temperature in Penang today in the early morning is 26  C.. If you take into account factors such as wind, humidity and other weather conditions, the temperatures can feel like 28  C.. The chance of rain in Penang in the morning is 71%, and the wind will blow at 9 km/h., title: Penang local weather (live): today, hourly weather, link: https://www.weather25.com/asia/malaysia/pulau-pinang/penang?page=today, snippet: Hourly Local Weather Forecast, weather conditions, precipitation, dew point, humidity, wind from Weather.com and The Weather Channel, title: Penang, Penang, Malaysia Weather - The Weather Channel, link: https://weather.com/weather/hourbyhour/l/Penang+Penang+Malaysia?placeId=ea7cdde5b43ac06d6171800fe18325742026c3741cc6a66aada6eeb4c136f6ba, snippet: Penang Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the Penang area., title: Penang, Malaysia Weather Conditions | Weather Underground, link: https://www.wunderground.com/weather/my/penang, snippet: Hourly Local Weather Forecast, weather ... Today. Hourly. 10 Day. Radar. Severe. Hourly Weather-Kepala Batas, Penang, Malaysia. As of 10:13 am GMT+08:00. Rain. Thunderstorms likely to continue for ..., title: Kepala Batas, Penang, Malaysia Weather - The Weather Channel, link: https://weather.com/weather/hourbyhour/l/Kepala+Batas+Penang+Malaysia?canonicalCityId=4f5145e7e316f70974b9abdcb2b88b4eState: [HumanMessage(content='check weather today in Penang', additional_kwargs={}, response_metadata={}, id='b5e1f6d0-14a6-4569-b72d-df56d3b37267'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-05-04T09:46:44.1675845Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1017235000, 'load_duration': 18264900, 'prompt_eval_count': 186, 'prompt_eval_duration': 230925600, 'eval_count': 23, 'eval_duration': 767524000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-a91bb088-5e62-4a7c-88dc-8ade6eb926b6', tool_calls=[{'name': 'duckduckgo_results_json', 'args': {'query': 'weather penang today'}, 'id': '32925990-282d-4e5e-b7a3-8d2eb3834806', 'type': 'tool_call'}], usage_metadata={'input_tokens': 186, 'output_tokens': 23, 'total_tokens': 209}), ToolMessage(content='snippet: The temperature in Penang today in the early morning is 26  C.. If you take into account factors such as wind, humidity and other weather conditions, the temperatures can feel like 28  C.. The chance of rain in Penang in the morning is 71%, and the wind will blow at 9 km/h., title: Penang local weather (live): today, hourly weather, link: https://www.weather25.com/asia/malaysia/pulau-pinang/penang?page=today, snippet: Hourly Local Weather Forecast, weather conditions, precipitation, dew point, humidity, wind from Weather.com and The Weather Channel, title: Penang, Penang, Malaysia Weather - The Weather Channel, link: https://weather.com/weather/hourbyhour/l/Penang+Penang+Malaysia?placeId=ea7cdde5b43ac06d6171800fe18325742026c3741cc6a66aada6eeb4c136f6ba, snippet: Penang Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the Penang area., title: Penang, Malaysia Weather Conditions | Weather Underground, link: https://www.wunderground.com/weather/my/penang, snippet: Hourly Local Weather Forecast, weather ... Today. Hourly. 10 Day. Radar. Severe. Hourly Weather-Kepala Batas, Penang, Malaysia. As of 10:13 am GMT+08:00. Rain. Thunderstorms likely to continue for ..., title: Kepala Batas, Penang, Malaysia Weather - The Weather Channel, link: https://weather.com/weather/hourbyhour/l/Kepala+Batas+Penang+Malaysia?canonicalCityId=4f5145e7e316f70974b9abdcb2b88b4e', name='duckduckgo_results_json', id='96bdc3e1-97e0-45d8-8663-f8262002d33f', tool_call_id='32925990-282d-4e5e-b7a3-8d2eb3834806', artifact=[{'snippet': 'The temperature in Penang today in the early morning is 26  C.. If you take into account factors such as wind, humidity and other weather conditions, the temperatures can feel like 28  C.. The chance of rain in Penang in the morning is 71%, and the wind will blow at 9 km/h.', 'title': 'Penang local weather (live): today, hourly weather', 'link': 'https://www.weather25.com/asia/malaysia/pulau-pinang/penang?page=today'}, {'snippet': 'Hourly Local Weather Forecast, weather conditions, precipitation, dew point, humidity, wind from Weather.com and The Weather Channel', 'title': 'Penang, Penang, Malaysia Weather - The Weather Channel', 'link': 'https://weather.com/weather/hourbyhour/l/Penang+Penang+Malaysia?placeId=ea7cdde5b43ac06d6171800fe18325742026c3741cc6a66aada6eeb4c136f6ba'}, {'snippet': 'Penang Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the Penang area.', 'title': 'Penang, Malaysia Weather Conditions | Weather Underground', 'link': 'https://www.wunderground.com/weather/my/penang'}, {'snippet': 'Hourly Local Weather Forecast, weather ... Today. Hourly. 10 Day. Radar. Severe. Hourly Weather-Kepala Batas, Penang, Malaysia. As of 10:13 am GMT+08:00. Rain. Thunderstorms likely to continue for ...', 'title': 'Kepala Batas, Penang, Malaysia Weather - The Weather Channel', 'link': 'https://weather.com/weather/hourbyhour/l/Kepala+Batas+Penang+Malaysia?canonicalCityId=4f5145e7e316f70974b9abdcb2b88b4e'}])] \n",
      "\n",
      "Here's the formatted answer:\n",
      "\n",
      "**Current Weather in Penang Today**\n",
      "\n",
      "The temperature in Penang today is around 26  C. Considering factors like wind and humidity, it feels more like 28  C.\n",
      "\n",
      "**Chance of Rain**\n",
      "\n",
      "There's a high chance of rain in Penang today, with a probability of 71%. The wind speed is expected to be around 9 km/h.\n",
      "\n",
      "**Hourly Weather Forecast**\n",
      "\n",
      "You can check the latest hourly weather forecast for Penang on websites like Weather.com and The Weather Channel. Additionally, you can also check the local weather conditions on Weather Underground's website.\n",
      "\n",
      "**Current Weather Conditions in Kepala Batas, Penang**\n",
      "\n",
      "As of 10:13 am GMT+08:00, there's a chance of thunderstorms continuing to affect the area. You can check the latest weather updates for Kepala Batas on The Weather Channel's website.\n",
      "\n",
      "Please note that the weather forecast is subject to change and may not be accurate at the time of reading.Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for msg, metadata in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, stream_mode=\"messages\"):\n",
    "        # if msg.content and metadata[\"langgraph_node\"] == \"chatbot_with_tools\":\n",
    "        if msg.content:\n",
    "            print(msg.content, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# def stream_graph_updates(user_input: str):\n",
    "#     for msg, metadata in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, stream_mode=\"messages\"):\n",
    "#         if isinstance(msg, BaseMessage) and hasattr(msg, \"content\"):\n",
    "#             print(msg.content or \"\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ddg_search = DuckDuckGoSearchResults()\n",
    "tools = [ddg_search]\n",
    "llm = ChatOllama(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model='qwen3:4b',\n",
    "    temperature=0.5\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# llm = ChatOllama(model=\"llama3.2:3b\", temperature=0).bind_tools([])\n",
    "for chunk in llm_with_tools.stream(\"Tell me a joke\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Agent streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    # model='qwen3:4b',\n",
    "    temperature=0.5\n",
    ")\n",
    "agent_executor = create_react_agent(model, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, metadata in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):\n",
    "        print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model='qwen3:4b',\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    llm_response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"Generate a joke about {state['topic']}\"}\n",
    "        ]\n",
    "    )\n",
    "    return {\"joke\": llm_response.content}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    # .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"generate_joke\")\n",
    "    # .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "for message_chunk, metadata in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if message_chunk.content:\n",
    "        print(message_chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to basic tool calling - testing streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model='qwen2.5:3b-instruct-q5_K_M',\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'fe01501d-a4cf-4404-a428-1d4438d636db', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'e7e3b3e8-2c69-4755-9ed9-2e22d21ed8cb', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(ai_msg.tool_calls)\n",
    "\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen2.5:3b-instruct-q5_K_M', 'created_at': '2025-05-04T09:56:36.6614518Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5338097200, 'load_duration': 3121257700, 'prompt_eval_count': 238, 'prompt_eval_duration': 511141700, 'eval_count': 53, 'eval_duration': 1697613700, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-3ee4290b-994f-4a45-863b-90a25f683b25-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '3df41bb5-e131-4272-a6a9-6e3dde6854d4', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': '604953ff-351e-4548-9be9-07cfe6f8cc04', 'type': 'tool_call'}], usage_metadata={'input_tokens': 238, 'output_tokens': 53, 'total_tokens': 291}),\n",
       " ToolMessage(content='36', name='multiply', tool_call_id='3df41bb5-e131-4272-a6a9-6e3dde6854d4'),\n",
       " ToolMessage(content='60', name='add', tool_call_id='604953ff-351e-4548-9be9-07cfe6f8cc04')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
    "            AIMessage(content='')]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? Feel free to ask me any questions or start a conversation on topics that interest you. Whether it's general knowledge, fun facts, or if you need help with something specific, just let me know.\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "response = llm.stream(messages)\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(f\"\\n{type(response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:3b-instruct\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "['c1f853ea-10df-49fd-af74-8680bbfd8707', '1720e2db-9a84-42d4-9e02-74eb689a545e', 'e266367f-b659-4ec5-8920-d00bc52dba31', '3842e83a-46f5-41ca-9a8c-df2b9be5ce23', '5f559d0a-6c60-4082-acf1-982be146687e', 'be424417-b6b6-4dd8-92a8-c9873387328f', 'c0f09fc0-93bf-4f7a-9a94-c25dbf5caddf', '2d153410-d2b7-42b6-a360-28e827ceb87e', '8a45e35f-1a8b-43d5-9d55-33c13e668fed', 'e9b1dea8-4125-45ad-8681-5eff02931fd3', '49d7b782-6ae2-4e7f-95a7-062f8cb921e9', '3098cd87-546a-43e1-a474-f18c2ede3644', '304e76d8-ff07-4bcc-91ba-9fc1c4c5a5fa', '1133987b-3f1c-496d-86a1-1fd76f5b100d', 'c9324b68-3fcb-4217-81bc-fee356b931ae', 'e8a98617-f85c-4ccf-8331-fdd9f4006a9d', '2c837f1a-aee5-42ee-b42f-45f314b2a170', '002e9332-097a-4569-ba15-a9f5241b4567', '2234563a-c71d-429b-98a5-d3473e5c1ae9', 'ed73074d-c19b-4589-8b21-e231b79f023f', 'ca250a95-6d0b-4ce2-b77b-b2de3475e7f5', 'f1c5eefe-c415-48c8-9c48-0711c601dcd7', 'caa5d0e1-1f71-409a-bda7-ba3a68014249', '2c11a719-bfaf-462b-86ad-4620e913a8af', 'a5bae790-748b-44ad-88e9-8d4dd867e370', '87e8a850-60eb-4e71-a5e5-df140dc92dd5', 'e129c151-365b-4ee7-9eaf-7f40587c9629', '4769cfb5-305b-4533-849a-ea1ea39b24f4', '86a7a7f0-1c2f-4ddd-9b5b-915a0b106237', '7dca42a9-305c-4172-94e7-e0875ca6d591', 'c73fe8d7-dd9f-4496-aa58-3aaf91108fd9', '93b5e908-f7e5-4520-a6ab-12b2911a4c0d', '4e41a487-ca35-452e-b7e1-9b97253e16b2', 'f91a3420-d3db-49d7-8ddc-b0cf3054686c', '9d1d1d64-ed50-4f43-ae96-80fae300803d', '9f752010-6d26-4e9f-a789-786f34cc1e8d', 'b747c09e-8843-44cd-aaf9-6cbf1c9904e6', 'e3ff11b1-8aa5-4c60-8603-126f62eaed07', '7c18ed04-9e57-4a5e-b1ec-d3b0a6bb40bd', 'c626fbe0-84b5-494e-9057-c005317c74be', 'cb46b31b-cbc3-4aeb-9c1d-c939dc3cc32b', '5c10e314-2332-4fa6-bb2c-c32eb1e316ae', 'd0e719a4-f63c-44fe-8f28-88469c787192', 'c3adb304-5854-44b9-90dd-47c30191b790', '4fd6a4ea-8d30-409a-b4ec-93632a83aa13', '2ffc38ca-b8dc-4f00-92f2-da710bfe38ab', '7d67d495-6919-4746-b351-22e099f25cef', 'b4257dbe-d203-4afe-8d97-493441ee46e8', '139e59de-69da-4896-99e8-e02d8a83c155', 'fa5f284c-5e47-4919-b83d-cafcc5f4cb62', '5a167d9a-9225-441b-ad89-087a14c42d9b', '5a12a79b-6df4-4550-9fb1-6aad8052079f', '8472e86a-84d0-40b7-b767-de07bb440c8c', '791057c3-a49b-4444-abc5-a897972197ff', '6b4c0b59-21dd-48dd-9bb6-3de6319c7f30', '228551d9-329b-4de6-97e6-2fa07acb92be', '80e167f6-380f-4a96-b75f-a4ab59b27955', 'f55a0d35-9ecc-4f7a-9e93-82787154ea99', '07d9e966-910a-4209-ad64-29e9ef82f07b', 'bbd58daf-c67f-4d4a-b83c-7e3e567375a7', '49af6d8e-ac33-47b0-879f-41c6f2e28038', 'ab20fbc8-b480-4ce1-b3ed-3a8940b8dc8c', 'dc5a8502-765e-4b4d-9d08-d342aedd6314']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(len(document_ids))\n",
    "print(document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayke\\Desktop\\Documents\\Programming\\Side Projects\\genai-chatbot-project\\.venv\\Lib\\site-packages\\langsmith\\client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='e266367f-b659-4ec5-8920-d00bc52dba31', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to think step by step to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the models thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='3842e83a-46f5-41ca-9a8c-df2b9be5ce23', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into Problem PDDL, then (2) requests a classical planner to generate a PDDL plan based on an existing Domain PDDL, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(id='87e8a850-60eb-4e71-a5e5-df140dc92dd5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19303}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\"), Document(id='2c11a719-bfaf-462b-86ad-4620e913a8af', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17734}, page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.')]\n",
      "\n",
      "\n",
      "Answer: Task Decomposition is a method where complex tasks are broken down into smaller, more manageable steps. This approach allows an agent or model to understand and execute large tasks by first decomposing them into multiple simpler sub-tasks. Techniques like Chain of Thought (CoT) guide models through this process step-by-step, while Tree of Thoughts expands on CoT by exploring various reasoning paths at each step.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='e266367f-b659-4ec5-8920-d00bc52dba31', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to think step by step to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the models thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='3842e83a-46f5-41ca-9a8c-df2b9be5ce23', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into Problem PDDL, then (2) requests a classical planner to generate a PDDL plan based on an existing Domain PDDL, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(id='87e8a850-60eb-4e71-a5e5-df140dc92dd5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19303}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\"), Document(id='2c11a719-bfaf-462b-86ad-4620e913a8af', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17734}, page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'Task Decomposition is a technique where complex tasks are broken down into smaller, more manageable subtasks. This approach helps in understanding and executing large tasks by transforming them into multiple simpler steps that can be handled individually. Methods like Chain of Thought (CoT) and Tree of Thoughts (ToT) utilize this method to decompose problems into thought steps and generate multiple possibilities for each step, aiding in the interpretation of model thinking processes.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task| Decom|position| refers| to| breaking| down| complex| tasks| into| smaller|,| more| manageable| sub|tasks|.| This| approach| helps| in| understanding| and| executing| complicated| instructions| by| guiding| models| to| think| step|-by|-step| and| decom|pose| problems| into| multiple| steps| or| sub|goals|.||"
     ]
    }
   ],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 8,\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "\n",
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 37831, 'section': 'end'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embedding_model)\n",
    "_ = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'query': 'What does the end of the post say about Task Decomposition?', 'section': 'end'}}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='8f8ebab5-f36b-4d94-8342-b90928c769e0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32858, 'section': 'end'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='e5c4f739-569c-44ca-a35b-f7dceddf7488', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 30868, 'section': 'end'}, page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",'), Document(id='1b655efa-17ee-47a7-9bac-345ffad85b69', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 31348, 'section': 'end'}, page_content='},\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",'), Document(id='e1fff527-0bae-44a2-95fa-0e5bd751f2d3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 40518, 'section': 'end'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. Chain of thought prompting elicits reasoning in large language models. NeurIPS 2022\\n[2] Yao et al. Tree of Thoughts: Dliberate Problem Solving with Large Language Models. arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. Chain of Hindsight Aligns Language Models with Feedback\\n arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. LLM+P: Empowering Large Language Models with Optimal Planning Proficiency arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. ReAct: Synergizing reasoning and acting in language models. ICLR 2023.\\n[6] Google Blog. Announcing ScaNN: Efficient Vector Similarity Search July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'The end of the post does not provide information about Task Decomposition. It discusses agent behavior in a conversation for task clarification and mentions references to articles related to large language model usage, but it does not address Task Decomposition directly.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition? think carefully of which section to retrieve\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
